{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "going-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#done done done ..... most important,in this the in the main page there was a articles and pdf as well\n",
    "# the articles link was to different .so i used split method to convert the string into the list and get the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "soviet-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stepi_re_kr_publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hired-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.stepi.re.kr/site/stepien/ex/bbs/List.do?pageIndex=1&cbIdx=1303&bcIdx=0&reIdx=0&cateCont=&searchSort=REG_DT&category=&tgtTypeCd=SUB_CONT&searchKey="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.stepi.re.kr/site/stepien/ex/bbs/publicationView.do?pageIndex=1&cbIdx=1310&bcIdx=36978&reIdx=0&cateCont=CM0233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "offshore-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_pattern(<em class=\"cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sensitive-status",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=45&cateCont=A0508&streFileNm=9c19a5c2-f479-45ee-81d4-0968a45bba50.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=44&cateCont=A0508&streFileNm=94bb69c7-25c8-44d9-813b-3276d88cfff0.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=43&cateCont=A0508&streFileNm=888cc606-c31a-424b-892e-b4acd2002d89.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=42&cateCont=A0508&streFileNm=e8c14ffd-cbcc-485d-98c3-1461cc2bcfde.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=41&cateCont=A0508&streFileNm=af37d9a3-74ef-4072-bb6d-0c7888f7de13.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=40&cateCont=A0508&streFileNm=8995ec7b-365b-4c6f-80a1-5db65f7ded1a.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=39&cateCont=A0508&streFileNm=d10a67de-9ed0-4c69-906d-2a7c4e09a184.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=38&cateCont=A0508&streFileNm=7b854591-f159-4b5f-b79c-c416883eea5a.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=37&cateCont=A0508&streFileNm=b8963656-b2d0-4643-bc00-453dc9d3a282.pdf\n",
      "https://www.stepi.re.kr/common/report/Download.do?reIdx=36&cateCont=A0508&streFileNm=a554b6b0-e24c-4dd3-b11d-a91261c55ef3.pdf\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "from datetime import date\n",
    "import re\n",
    "import sys\n",
    "import urllib, urllib.request, urllib.parse\n",
    "import random\n",
    "from scrawl import *\n",
    "\n",
    "# Date and time\n",
    "start_time = time.time()\n",
    "current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "created_on = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# client_id = sys.argv[1]\n",
    "client_id = '5f69d22ef472d6646f577fa6'  # Europe\n",
    "site = 'stepi_re_kr_publication'\n",
    "site_name = 'Science and Technology Policy Institute (STEPI) (Republic of Korea)'\n",
    "c = Crawl()  # creating obje_\n",
    "# create directories to store logs.\n",
    "log_path = c.create_directories(project_path, client_id, site)\n",
    "\n",
    "# create image directories\n",
    "image_directory = c.create_image_directories(project_path)\n",
    "# creating pdf directories\n",
    "pdf_directory = c.create_pdf_directories(project_path, site)\n",
    "# logger\n",
    "logger = log_func(log_path, created_on, current_time)\n",
    "logger.info(\"Process Started ...\\n\")\n",
    "\n",
    "# initialize variables\n",
    "skipped_due_to_headline = 0\n",
    "skipped_due_to_content = 0\n",
    "skipped_due_to_date = 0\n",
    "missing_overall_tonality = 0\n",
    "no_of_data = 0\n",
    "duplicate_data = 0  \n",
    "unable_to_fetch_article_url = 0\n",
    "unable_to_download_pdf = 0\n",
    "publish_source = 'stepi.re.kr'\n",
    "country = 'Korea'\n",
    "language = 'English'\n",
    "images_path = []\n",
    "\n",
    "home_page = c.download_page('https://www.stepi.re.kr/site/stepien/ex/bbs/List.do?cbIdx=1303')\n",
    "\n",
    "for i in home_page.split('<em class=\"cate')[1:]:\n",
    "    # source_link\n",
    "    source_link = c.scrap('<a\\s*href=\"(.*?)\"', i).strip() \n",
    "\n",
    "    source_link = re.sub ('\\s+','',source_link)  # to remove the unwanted space(MORE THAN 1 SPACE)\n",
    "    if not source_link.endswith('.pdf'):\n",
    "        source_link = c.scrap('doBbsFView\\((.*?)\\)',source_link).split(',')  # WILL GET IN THE STRING SO BAAD MAI EK EK INDEX LEYNE KE LIA SPLIT KARNA PADEYGA\n",
    "        source_link = f'https://www.stepi.re.kr/site/stepien/ex/bbs/publicationView.do?pageIndex=1&cbIdx={source_link[0]}&bcIdx={source_link[1]}&reIdx=0&cateCont={source_link[2]}'\n",
    "        source_link = re.sub(\"'\",'',source_link)\n",
    "\n",
    "    if 'http' not in source_link:\n",
    "        source_link = 'https://www.stepi.re.kr' + source_link\n",
    "    print(source_link)\n",
    "#     continue\n",
    "    # handle duplicates\n",
    "    source_link_query = {'source_link':source_link}\n",
    "    dic = cl_data.find_one(source_link_query,{'source_link': 1}) \n",
    "    if dic:\n",
    "        duplicate_data += 1\n",
    "        continue\n",
    "\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    logger.info(f'Fetching {source_link}\\n')\n",
    "    if '.pdf' not in source_link:\n",
    "        page = c.download_page(source_link)   \n",
    "        if page.startswith('Unable to fetch'):\n",
    "            logger.info(page)\n",
    "            unable_to_fetch_article_url += 1\n",
    "            continue    \n",
    "\n",
    "    source_headline = c.scrap('<span\\s*class=\"title\">(.*?)</span>', i)\n",
    "\n",
    "    # skip if headline not found\n",
    "    if not source_headline:\n",
    "        logger.info(f'Skipping due to headline {source_link}\\n')\n",
    "        skipped_due_to_headline += 1\n",
    "        continue\n",
    "\n",
    "     # Date and time\n",
    "    pub_date, publish_time = '', current_time\n",
    "\n",
    "    try:   \n",
    "        date_time_str = c.scrap('<li><b>DATE\\s*:\\s*</b>(.*?)</li>', i)\n",
    "        date_time_str = re.sub('&.*?;','',date_time_str)\n",
    "        date_time_str = re.sub('[^\\w+]', '', date_time_str)  \n",
    "        date_time_obj = datetime.strptime(date_time_str, '%Y%m%d')#2021-04-09T00:00:00\n",
    "        ist_date_time = date_time_obj - timedelta(hours = 3,minutes = 30)  \n",
    "        ist_date_time = ist_date_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        pub_date = ist_date_time[:10]\n",
    "        publish_time = ist_date_time[11:]\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # skip null date\n",
    "    if not pub_date:\n",
    "        logger.info(f'Skipping due to date {source_link}\\n')            \n",
    "        skipped_due_to_date += 1\n",
    "        continue\n",
    "\n",
    "    # break if date is not today's date\n",
    "#     if pub_date != created_on:\n",
    "#         break    \n",
    "\n",
    "    # source_content\n",
    "    if '.pdf' not in source_link:\n",
    "        source_content = c.scrap('<div\\s*class=\"viewCon\">(.*?)<div\\s*class=\"boardBtm\">',page)\n",
    "    else:\n",
    "        source_content = source_headline\n",
    "    source_content = re.sub('&.*?;','',source_content,re.S)\n",
    "    source_content = c.strip_html(source_content)\n",
    "    if not source_content:\n",
    "        logger.info(f'Skipping due to content {source_link}\\n')            \n",
    "        skipped_due_to_content += 1\n",
    "        continue\n",
    "\n",
    "    if '.pdf' in source_link:\n",
    "        journalist =c.scrap(\"'author-name':'(.*?)'\",i)\n",
    "        if not journalist: journalist = 'NA'\n",
    "    else:\n",
    "        journalist =c.scrap(\"'author-name':'(.*?)'\",page)\n",
    "        if not journalist: journalist = 'NA'\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    # current date and time 00\n",
    "    harvest_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    # temp link\n",
    "    temp_link = source_link\n",
    "\n",
    "    # headline and content \n",
    "    headline = source_headline\n",
    "    content = source_content\n",
    "\n",
    "    # overall_tonality\n",
    "    overall_tonality = ''\n",
    "\n",
    "    # word count\n",
    "    word_count = len((source_headline + ' ' + source_content).split())\n",
    "\n",
    "    html_content = ''\n",
    "\n",
    "    # image_urls\n",
    "    image_urls = []\n",
    "    \n",
    "    pdf_path, pdf_name = '', ''\n",
    "    if '.pdf' in source_link:\n",
    "        pdf_url = source_link\n",
    "        pdf_name = c.scrap('.*\\-(.*)',pdf_url)\n",
    "        # pdf_path\n",
    "        pdf_path = f'{pdf_directory}/{pdf_name}'        \n",
    "\n",
    "        # download pdf\n",
    "        pdf = c.download_pdf(pdf_url, pdf_path)\n",
    "        if pdf.startswith('Unable to fetch'):\n",
    "            logger.info(pdf) # writes error message with error code\n",
    "            unable_to_download_pdf += 1\n",
    "            continue\n",
    "  \n",
    "    # storing the above data in a dictionary\n",
    "    clientdata ={\n",
    "                    \"client_master\" : client_id, \n",
    "                    \"articleid\":client_id,\n",
    "                    \"medium\":'Web' ,\n",
    "                    \"searchkeyword\":[],\n",
    "                    \"entityname\" : [] ,\n",
    "                    \"process_flage\":\"1\",\n",
    "                    \"na_flage\":\"0\",\n",
    "                    \"na_reason\":\"\",\n",
    "                    \"qc_by\":\"\",\n",
    "                    \"qc_on\":\"\",\n",
    "                    \"location\":\"\",\n",
    "                    \"spokeperson\":\"\",\n",
    "                    \"quota\":\"\",\n",
    "                    \"overall_topics\":\"\",\n",
    "                    \"person\":\"\",\n",
    "                    \"overall_entites\":\"\",\n",
    "                    \"overall_tonality\": overall_tonality,\n",
    "                    \"overall_wordcount\":word_count,\n",
    "                    \"article_subjectivity\":\"\",\n",
    "                    \"article_summary\":\"\",\n",
    "                    \"pub_date\":pub_date,\n",
    "                    \"publish_time\":publish_time,\n",
    "                    \"harvest_time\":harvest_time,\n",
    "                    \"temp_link\":temp_link,\n",
    "                    \"publish_source\": publish_source,\n",
    "                    \"programme\":'null',\n",
    "                    \"feed_class\":\"News\",\n",
    "                    \"publishing_platform\":\"\",\n",
    "                    \"klout_score\":\"\",\n",
    "                    \"journalist\":journalist,\n",
    "                    \"headline\":headline,\n",
    "                    \"content\":content,\n",
    "                    \"source_headline\":source_headline,\n",
    "                    \"source_content\":source_content,\n",
    "                    \"language\":language,\n",
    "                    \"presence\":'null',\n",
    "                    \"clip_type\":'null',\n",
    "                    \"prog_slot\":'null',\n",
    "                    \"op_ed\":'0',\n",
    "                    \"location_mention\":'',\n",
    "                    \"source_link\":source_link,\n",
    "                    \"author_contact\":'',\n",
    "                    \"author_emailid\":'',\n",
    "                    \"author_url\":'',\n",
    "                    \"city\":'',\n",
    "                    \"state\":'',\n",
    "                    \"country\":country,\n",
    "                    \"source\":publish_source,\n",
    "                    \"foot_fall\":'',\n",
    "                    \"created_on\":created_on,\n",
    "                    \"active\":'1',\n",
    "                    'crawl_flag':2,\n",
    "                    \"images_path\":images_path,\n",
    "                    \"html_content\":html_content,\n",
    "                    \"pdf_url\": pdf_url,\n",
    "                    \"pdf_name\": pdf_name,\n",
    "                    \"pdf_path\":pdf_path\n",
    "                }\n",
    "\n",
    "#     cl_data.insert_one(clientdata)  \n",
    "    no_of_data += 1\n",
    "logger.info('Iteration complete\\n')   \n",
    "logger.info(f'Number of data: {no_of_data}\\n')\n",
    "logger.info(f'Duplicate data: {duplicate_data}\\n')\n",
    "logger.info(f'Unable to fetch article url: {unable_to_fetch_article_url}\\n')\n",
    "logger.info(f'Skipped due to headline: {skipped_due_to_headline}\\n')\n",
    "logger.info(f'Unable to download pdf: {unable_to_download_pdf}\\n')\n",
    "logger.info(f'Skipped due to content: {skipped_due_to_content}\\n')\n",
    "logger.info(f'Skipped due to date: {skipped_due_to_date}\\n')\n",
    "logger.info(f'country: {country}\\n')\n",
    "logger.info(f'language: {language}\\n')\n",
    "logger.info(f'Processing finished in {time.time() - start_time} seconds.\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-ranch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "from datetime import date\n",
    "import re\n",
    "import sys\n",
    "import urllib, urllib.request, urllib.parse\n",
    "import random\n",
    "from scrawl import *\n",
    "\n",
    "# Date and time\n",
    "start_time = time.time()\n",
    "current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "created_on = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# client_id = sys.argv[1]\n",
    "client_id = '5f69d22ef472d6646f577fa6'  \n",
    "site = 'stepi_re_kr_publication'\n",
    "site_name = 'Science and Technology Policy Institute (STEPI) (Republic of Korea)'\n",
    "c = Crawl()  # creating obje_\n",
    "# create directories to store logs.\n",
    "log_path = c.create_directories(project_path, client_id, site)\n",
    "\n",
    "# create image directories\n",
    "image_directory = c.create_image_directories(project_path)\n",
    "# creating pdf directories\n",
    "pdf_directory = c.create_pdf_directories(project_path, site)\n",
    "# logger\n",
    "logger = log_func(log_path, created_on, current_time)\n",
    "logger.info(\"Process Started ...\\n\")\n",
    "\n",
    "# initialize variables\n",
    "skipped_due_to_headline = 0\n",
    "skipped_due_to_content = 0\n",
    "skipped_due_to_date = 0\n",
    "missing_overall_tonality = 0\n",
    "no_of_data = 0\n",
    "duplicate_data = 0  \n",
    "unable_to_fetch_article_url = 0\n",
    "unable_to_download_pdf = 0\n",
    "publish_source = 'stepi.re.kr'\n",
    "country = 'Korea'\n",
    "language = 'English'\n",
    "images_path = []\n",
    "foot_fall = c.get_foot_fall(publish_source)\n",
    "\n",
    "home_page = c.download_page('https://www.stepi.re.kr/site/stepien/ex/bbs/List.do?cbIdx=1303')\n",
    "\n",
    "for i in home_page.split('<em class=\"cate')[1:]:\n",
    "    # source_link\n",
    "    source_link = c.scrap('<a\\s*href=\"(.*?)\"', i).strip() \n",
    "    source_link = re.sub ('\\s+','',source_link) \n",
    "    if not source_link.endswith('.pdf'):\n",
    "        source_link = c.scrap('doBbsFView\\((.*?)\\)',source_link).split(',')\n",
    "        source_link = f'https://www.stepi.re.kr/site/stepien/ex/bbs/publicationView.do?pageIndex=1&cbIdx={source_link[0]}&bcIdx={source_link[1]}&reIdx=0&cateCont={source_link[2]}'\n",
    "        source_link = re.sub(\"'\",'',source_link)\n",
    "\n",
    "    if 'http' not in source_link:\n",
    "        source_link = 'https://www.stepi.re.kr' + source_link\n",
    "\n",
    "    # handle duplicates\n",
    "    source_link_query = {'source_link':source_link}\n",
    "    dic = cl_data.find_one(source_link_query,{'source_link': 1}) \n",
    "    if dic:\n",
    "        duplicate_data += 1\n",
    "        continue\n",
    "\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    logger.info(f'Fetching {source_link}\\n')\n",
    "    if '.pdf' not in source_link:\n",
    "        page = c.download_page(source_link)   \n",
    "        if page.startswith('Unable to fetch'):\n",
    "            logger.info(page)\n",
    "            unable_to_fetch_article_url += 1\n",
    "            continue    \n",
    "\n",
    "    source_headline = c.scrap('<span\\s*class=\"title\">(.*?)</span>', i)\n",
    "\n",
    "    # skip if headline not found\n",
    "    if not source_headline:\n",
    "        logger.info(f'Skipping due to headline {source_link}\\n')\n",
    "        skipped_due_to_headline += 1\n",
    "        continue\n",
    "\n",
    "     # Date and time\n",
    "    pub_date, publish_time = '', current_time\n",
    "\n",
    "    try:   \n",
    "        date_time_str = c.scrap('<li><b>DATE\\s*:\\s*</b>(.*?)</li>', i)\n",
    "        date_time_str = re.sub('&.*?;','',date_time_str)\n",
    "        date_time_str = re.sub('[^\\w+]', '', date_time_str)  \n",
    "        date_time_obj = datetime.strptime(date_time_str, '%Y%m%d')\n",
    "        ist_date_time = date_time_obj - timedelta(hours = 3,minutes = 30)  \n",
    "        ist_date_time = ist_date_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        pub_date = ist_date_time[:10]\n",
    "        publish_time = ist_date_time[11:]\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # skip null date\n",
    "    if not pub_date:\n",
    "        logger.info(f'Skipping due to date {source_link}\\n')            \n",
    "        skipped_due_to_date += 1\n",
    "        continue\n",
    "\n",
    "    # break if date is not today's date\n",
    "#     if pub_date != created_on:\n",
    "#         break    \n",
    "\n",
    "    # source_content\n",
    "    if '.pdf' not in source_link:\n",
    "        source_content = c.scrap('<div\\s*class=\"viewCon\">(.*?)<div\\s*class=\"boardBtm\">',page)\n",
    "    else:\n",
    "        source_content = source_headline\n",
    "    source_content = re.sub('&.*?;','',source_content,re.S)\n",
    "    source_content = c.strip_html(source_content)\n",
    "    if not source_content:\n",
    "        logger.info(f'Skipping due to content {source_link}\\n')            \n",
    "        skipped_due_to_content += 1\n",
    "        continue\n",
    "\n",
    "\n",
    "    journalist =c.scrap(\"'author-name':'(.*?)'\",page)\n",
    "\n",
    "    if not journalist: journalist = 'NA'\n",
    "\n",
    "    # current date and time 00\n",
    "    harvest_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    # temp link\n",
    "    temp_link = source_link\n",
    "\n",
    "    # headline and content \n",
    "    headline = source_headline\n",
    "    content = source_content\n",
    "\n",
    "    # overall_tonality\n",
    "    overall_tonality = ''\n",
    "\n",
    "    # word count\n",
    "    word_count = len((source_headline + ' ' + source_content).split())\n",
    "\n",
    "    html_content = ''\n",
    "\n",
    "    # image_urls\n",
    "    image_urls = []\n",
    "    \n",
    "    pdf_path, pdf_name = '', ''\n",
    "    if '.pdf' in source_link:\n",
    "        pdf_url = source_link\n",
    "        pdf_name = c.scrap('.*\\-(.*)',pdf_url)\n",
    "        # pdf_path\n",
    "        pdf_path = f'{pdf_directory}/{pdf_name}'        \n",
    "\n",
    "        # download pdf\n",
    "        pdf = c.download_pdf(pdf_url, pdf_path)\n",
    "        if pdf.startswith('Unable to fetch'):\n",
    "            logger.info(pdf) # writes error message with error code\n",
    "            unable_to_download_pdf += 1\n",
    "            continue\n",
    "  \n",
    "    # storing the above data in a dictionary\n",
    "    clientdata ={\n",
    "                    \"client_master\" : client_id, \n",
    "                    \"articleid\":client_id,\n",
    "                    \"medium\":'Web' ,\n",
    "                    \"searchkeyword\":[],\n",
    "                    \"entityname\" : [] ,\n",
    "                    \"process_flage\":\"1\",\n",
    "                    \"na_flage\":\"0\",\n",
    "                    \"na_reason\":\"\",\n",
    "                    \"qc_by\":\"\",\n",
    "                    \"qc_on\":\"\",\n",
    "                    \"location\":\"\",\n",
    "                    \"spokeperson\":\"\",\n",
    "                    \"quota\":\"\",\n",
    "                    \"overall_topics\":\"\",\n",
    "                    \"person\":\"\",\n",
    "                    \"overall_entites\":\"\",\n",
    "                    \"overall_tonality\": overall_tonality,\n",
    "                    \"overall_wordcount\":word_count,\n",
    "                    \"article_subjectivity\":\"\",\n",
    "                    \"article_summary\":\"\",\n",
    "                    \"pub_date\":pub_date,\n",
    "                    \"publish_time\":publish_time,\n",
    "                    \"harvest_time\":harvest_time,\n",
    "                    \"temp_link\":temp_link,\n",
    "                    \"publish_source\": publish_source,\n",
    "                    \"programme\":'null',\n",
    "                    \"feed_class\":\"News\",\n",
    "                    \"publishing_platform\":\"\",\n",
    "                    \"klout_score\":\"\",\n",
    "                    \"journalist\":journalist,\n",
    "                    \"headline\":headline,\n",
    "                    \"content\":content,\n",
    "                    \"source_headline\":source_headline,\n",
    "                    \"source_content\":source_content,\n",
    "                    \"language\":language,\n",
    "                    \"presence\":'null',\n",
    "                    \"clip_type\":'null',\n",
    "                    \"prog_slot\":'null',\n",
    "                    \"op_ed\":'0',\n",
    "                    \"location_mention\":'',\n",
    "                    \"source_link\":source_link,\n",
    "                    \"author_contact\":'',\n",
    "                    \"author_emailid\":'',\n",
    "                    \"author_url\":'',\n",
    "                    \"city\":'',\n",
    "                    \"state\":'',\n",
    "                    \"country\":country,\n",
    "                    \"source\":publish_source,\n",
    "                    \"foot_fall\":foot_fall,\n",
    "                    \"created_on\":created_on,\n",
    "                    \"active\":'1',\n",
    "                    'crawl_flag':2,\n",
    "                    \"images_path\":images_path,\n",
    "                    \"html_content\":html_content,\n",
    "                    \"pdf_url\": pdf_url,\n",
    "                    \"pdf_name\": pdf_name,\n",
    "                    \"pdf_path\":pdf_path\n",
    "                }\n",
    "\n",
    "    cl_data.insert_one(clientdata)  \n",
    "    no_of_data += 1\n",
    "logger.info('Iteration complete\\n')   \n",
    "logger.info(f'Number of data: {no_of_data}\\n')\n",
    "logger.info(f'Duplicate data: {duplicate_data}\\n')\n",
    "logger.info(f'Unable to fetch article url: {unable_to_fetch_article_url}\\n')\n",
    "logger.info(f'Skipped due to headline: {skipped_due_to_headline}\\n')\n",
    "logger.info(f'Unable to download pdf: {unable_to_download_pdf}\\n')\n",
    "logger.info(f'Skipped due to content: {skipped_due_to_content}\\n')\n",
    "logger.info(f'Skipped due to date: {skipped_due_to_date}\\n')\n",
    "logger.info(f'country: {country}\\n')\n",
    "logger.info(f'language: {language}\\n')\n",
    "logger.info(f'Processing finished in {time.time() - start_time} seconds.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
