{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "infectious-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19-05-2021  # unknown error tabh aata hai pdf mai url sahi honey ke baad v jabh uska naam sahi sey save nhi hota  url name mai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    " # pdf ke download mai jabh unknown error ata hai so most probably wah naam ke wajah sey wajah sey ata hai ki file ka naam sahi dhang sey save nhi ho rha hai , jaise usmai .pdf naam sey nhi save ho rhahai\n",
    "    # esmai script mai wahi huwa ,unknown error aaaaaaa rha hai while downloading pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-burke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "therapeutic-trade",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/en/aboutcriepi/outline.html_ 0\n",
      "/en/publications/pamphletslabo/index.html_ 1\n",
      "/en/aboutcriepi/privacy.html_ 2\n",
      "/en/enic/index.html_ 3\n",
      "/en/publications/pamphletslabo/index.html_ 4\n",
      "/en/publications/pamphletslabo/index.html_ 5\n",
      "/en/aboutcriepi/outline.html_ 6\n",
      "/en/aboutcriepi/outline.html_ 7\n",
      "/en/publications/pamphletslabo/index.html_ 8\n",
      "/en/aboutcriepi/FY2018_AnnualReport.pdf?v2_ 9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "from datetime import date\n",
    "import re\n",
    "import sys\n",
    "import urllib, urllib.request, urllib.parse\n",
    "import random\n",
    "from scrawl import *\n",
    "\n",
    "# Date and time\n",
    "start_time = time.time()\n",
    "current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "created_on = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# client_id = sys.argv[1]\n",
    "client_id = '5f69d22ef472d6646f577fa6'  # Europe\n",
    "site = 'criepi_denken_or_jp_publication'\n",
    "site_name = 'Central Research Institute of Electric Power Industry (CRIEPI) (JAPAN)'\n",
    "c = Crawl()  # creating obje_\n",
    "# create directories to store logs.\n",
    "log_path = c.create_directories(project_path, client_id, site)\n",
    "\n",
    "# create image directories\n",
    "image_directory = c.create_image_directories(project_path)\n",
    "\n",
    "# creating pdf directories\n",
    "pdf_directory = c.create_pdf_directories(project_path, site)\n",
    "# logger\n",
    "logger = log_func(log_path, created_on, current_time)\n",
    "logger.info(\"Process Started ....\\n\")\n",
    "\n",
    "# initialize variables\n",
    "skipped_due_to_headline = 0\n",
    "skipped_due_to_content = 0\n",
    "skipped_due_to_date = 0\n",
    "missing_overall_tonality = 0\n",
    "no_of_data = 0\n",
    "duplicate_data = 0  \n",
    "unable_to_fetch_article_url = 0\n",
    "unable_to_download_pdf = 0\n",
    "publish_source = 'criepi.denken.or.jp'\n",
    "country = 'Japan'\n",
    "language = 'English'\n",
    "images_path = []\n",
    "\n",
    "home_page = c.download_page('https://criepi.denken.or.jp/en/')\n",
    "\n",
    "for i in home_page.split('<dl>')[1:]:\n",
    "    # source_link\n",
    "    source_link = c.scrap('<a\\s*href=\"(.*?)\"', i)  \n",
    "    if 'http' in source_link:\n",
    "        continue\n",
    "    if 'http' not in source_link:\n",
    "        source_link = 'https://criepi.denken.or.jp' + source_link\n",
    "    if 'en/enic/index.html' in source_link:\n",
    "        continue\n",
    "    print(source_link + \"_\" , count)\n",
    "    continue\n",
    "    # handle duplicates\n",
    "    source_link_query = {'source_link':source_link}\n",
    "    dic = cl_data.find_one(source_link_query,{'source_link': 1}) \n",
    "    if dic:\n",
    "        duplicate_data += 1\n",
    "        continue\n",
    "\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    logger.info(f'Fetching {source_link}\\n')\n",
    "    if '.pdf' not in source_link:\n",
    "        page = c.download_page(source_link)   \n",
    "        if page.startswith('Unable to fetch'):\n",
    "            logger.info(page)\n",
    "            unable_to_fetch_article_url += 1\n",
    "            continue \n",
    "    if '.pdf' in source_link:\n",
    "#         source_headline = c.scrap('/(.*?).pdf',source_link)  # in this it wil take the first match (/)\n",
    "        source_headline = c.scrap('.*/(.*?).pdf',source_link)  # it wil match the last match of the condition\n",
    "    else:        \n",
    "        source_headline = c.scrap('<title>(.*?)</title>', page)\n",
    "        source_headline = re.sub('&amp;','&',source_headline)\n",
    "        source_headline = re.sub('^CRIEPI|-|About CRIEPI','',source_headline).strip()  # to subsitute the starting of the occurance\n",
    "\n",
    "\n",
    "    # skip if headline not found\n",
    "    if not source_headline:\n",
    "        logger.info(f'Skipping due to headline {source_link}\\n')\n",
    "        skipped_due_to_headline += 1\n",
    "        continue\n",
    "\n",
    "     # Date and time\n",
    "    pub_date, publish_time = created_on , current_time\n",
    "\n",
    "    # skip null date\n",
    "    if not pub_date:\n",
    "        logger.info(f'Skipping due to date {source_link}\\n')            \n",
    "        skipped_due_to_date += 1\n",
    "        continue\n",
    "\n",
    "    # break if date is not today's date\n",
    "    if pub_date != created_on:\n",
    "        break    \n",
    "\n",
    "    # source_content\n",
    "    if '.pdf' not in source_link:\n",
    "        source_content = c.scrap('<div\\s*class=\"privacy-Box\">(.*?)<p\\s*class=\"page-top\">',page)\n",
    "        if not source_content:\n",
    "            source_content = source_headline          \n",
    "    else:\n",
    "        source_content = source_headline\n",
    "    source_content = re.sub('&.*?;','',source_content,re.S)\n",
    "    source_content = c.strip_html(source_content)\n",
    "\n",
    "    if not source_content:\n",
    "        logger.info(f'Skipping due to content {source_link}\\n')            \n",
    "        skipped_due_to_content += 1\n",
    "        continue\n",
    "    journalist = ''\n",
    "    if '.pdf' not in source_link:\n",
    "        journalist =c.scrap(\"'author-name':'(.*?)'\",page)\n",
    "\n",
    "    if not journalist: journalist = 'NA'\n",
    "\n",
    "    # current date and time 00\n",
    "    harvest_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    # temp link\n",
    "    temp_link = source_link\n",
    "\n",
    "    # headline and content \n",
    "    headline = source_headline\n",
    "    content = source_content\n",
    "\n",
    "    # overall_tonality\n",
    "    overall_tonality = ''\n",
    "\n",
    "    # word count\n",
    "    word_count = len((source_headline + ' ' + source_content).split())\n",
    "\n",
    "    html_content = ''\n",
    "\n",
    "    # image_urls\n",
    "    image_urls = []\n",
    "    \n",
    "    pdf_path, pdf_name = '', ''\n",
    "\n",
    "    if '.pdf' in source_link:\n",
    "        pdf_url = source_link\n",
    "        pdf_name = c.scrap('.*\\/(.*)\\?',pdf_url)  # agar name dhang sey nhi save hoga mean .pdf ending mai so unknown error ata hai mostly\n",
    "        pdf_path = f'{pdf_directory}/{pdf_name}'        \n",
    "        # download pdf\n",
    "        pdf = c.download_pdf(pdf_url, pdf_path)\n",
    "        if pdf.startswith('Unable to fetch'):\n",
    "            logger.info(pdf) # writes error message with error code\n",
    "            unable_to_download_pdf += 1\n",
    "            continue\n",
    "    else:\n",
    "        for i in page.split('href=')[1:]:\n",
    "            pdf_url = c.scrap('\"(.*?)\"',i)\n",
    "            if '.pdf' in pdf_url:\n",
    "                if ('komae.pdf' in pdf_url) or ('nuclear.pdf' in pdf_url) or ('system.pdf' in pdf_url):\n",
    "                    continue\n",
    "                if 'http' not in pdf_url:\n",
    "                    pdf_main_url = c.scrap('(.*)/',source_link)\n",
    "                    pdf_url = pdf_main_url + '/'+ pdf_url\n",
    "                    if '?' in pdf_url:\n",
    "                        pdf_name = c.scrap('.*\\/(.*)\\?',pdf_url)\n",
    "                    else:\n",
    "                        pdf_name = c.scrap('.*\\/(.*)',pdf_url)\n",
    "                # pdf_path\n",
    "                pdf_path = f'{pdf_directory}/{pdf_name}'        \n",
    "\n",
    "                # download pdf\n",
    "                pdf = c.download_pdf(pdf_url, pdf_path)\n",
    "                if pdf.startswith('Unable to fetch'):\n",
    "                    logger.info(pdf) # writes error message with error code\n",
    "                    unable_to_download_pdf += 1\n",
    "                    continue\n",
    "  \n",
    "    # storing the above data in a dictionary\n",
    "    clientdata ={\n",
    "                    \"client_master\" : client_id, \n",
    "                    \"articleid\":client_id,\n",
    "                    \"medium\":'Web' ,\n",
    "                    \"searchkeyword\":[],\n",
    "                    \"entityname\" : [] ,\n",
    "                    \"process_flage\":\"1\",\n",
    "                    \"na_flage\":\"0\",\n",
    "                    \"na_reason\":\"\",\n",
    "                    \"qc_by\":\"\",\n",
    "                    \"qc_on\":\"\",\n",
    "                    \"location\":\"\",\n",
    "                    \"spokeperson\":\"\",\n",
    "                    \"quota\":\"\",\n",
    "                    \"overall_topics\":\"\",\n",
    "                    \"person\":\"\",\n",
    "                    \"overall_entites\":\"\",\n",
    "                    \"overall_tonality\": overall_tonality,\n",
    "                    \"overall_wordcount\":word_count,\n",
    "                    \"article_subjectivity\":\"\",\n",
    "                    \"article_summary\":\"\",\n",
    "                    \"pub_date\":pub_date,\n",
    "                    \"publish_time\":publish_time,\n",
    "                    \"harvest_time\":harvest_time,\n",
    "                    \"temp_link\":temp_link,\n",
    "                    \"publish_source\": publish_source,\n",
    "                    \"programme\":'null',\n",
    "                    \"feed_class\":\"News\",\n",
    "                    \"publishing_platform\":\"\",\n",
    "                    \"klout_score\":\"\",\n",
    "                    \"journalist\":journalist,\n",
    "                    \"headline\":headline,\n",
    "                    \"content\":content,\n",
    "                    \"source_headline\":source_headline,\n",
    "                    \"source_content\":source_content,\n",
    "                    \"language\":language,\n",
    "                    \"presence\":'null',\n",
    "                    \"clip_type\":'null',\n",
    "                    \"prog_slot\":'null',\n",
    "                    \"op_ed\":'0',\n",
    "                    \"location_mention\":'',\n",
    "                    \"source_link\":source_link,\n",
    "                    \"author_contact\":'',\n",
    "                    \"author_emailid\":'',\n",
    "                    \"author_url\":'',\n",
    "                    \"city\":'',\n",
    "                    \"state\":'',\n",
    "                    \"country\":country,\n",
    "                    \"source\":publish_source,\n",
    "                    \"foot_fall\":'',\n",
    "                    \"created_on\":created_on,\n",
    "                    \"active\":'1',\n",
    "                    'crawl_flag':2,\n",
    "                    \"images_path\":images_path,\n",
    "                    \"html_content\":html_content,\n",
    "                    \"pdf_url\": pdf_url,\n",
    "                    \"pdf_name\": pdf_name,\n",
    "                    \"pdf_path\":pdf_path\n",
    "                }\n",
    "\n",
    "#     cl_data.insert_one(clientdata)  \n",
    "    no_of_data += 1\n",
    "logger.info('Iteration complete\\n')   \n",
    "logger.info(f'Number of data: {no_of_data}\\n')\n",
    "logger.info(f'Duplicate data: {duplicate_data}\\n')\n",
    "logger.info(f'Unable to fetch article url: {unable_to_fetch_article_url}\\n')\n",
    "logger.info(f'Skipped due to headline: {skipped_due_to_headline}\\n')\n",
    "logger.info(f'Unable to download pdf: {unable_to_download_pdf}\\n')\n",
    "logger.info(f'Skipped due to content: {skipped_due_to_content}\\n')\n",
    "logger.info(f'Skipped due to date: {skipped_due_to_date}\\n')\n",
    "logger.info(f'country: {country}\\n')\n",
    "logger.info(f'language: {language}\\n')\n",
    "logger.info(f'Processing finished in {time.time() - start_time} seconds.\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eastern-offer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FY2018_AnnualReport\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "a = 'https://criepi.denken.or.jp/en/aboutcriepi/FY2018_AnnualReport.pdf?v2'\n",
    "name = re.search('.*/(.*?).pdf',a).group(1)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-cricket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-uniform",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-briefs",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-recommendation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-stomach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
